{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: geopandas in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: pandas>=0.25.0 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from geopandas) (1.3.5)\n",
      "Requirement already satisfied: shapely>=1.6 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from geopandas) (1.7.0)\n",
      "Requirement already satisfied: fiona>=1.8 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from geopandas) (1.9.4.post1)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from geopandas) (3.2.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from fiona>=1.8->geopandas) (23.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from fiona>=1.8->geopandas) (2023.5.7)\n",
      "Requirement already satisfied: click~=8.0 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from fiona>=1.8->geopandas) (8.1.6)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from fiona>=1.8->geopandas) (6.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from pandas>=0.25.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from pandas>=0.25.0->geopandas) (1.21.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from click~=8.0->fiona>=1.8->geopandas) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\mnlee2\\appdata\\local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages (from importlib-metadata->fiona>=1.8->geopandas) (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install --upgrade geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'add_dll_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_30108\\3720369516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Define the arguments directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mnlee2\\AppData\\Local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages\\geopandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpoints_from_xy\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_read_file\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_file\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_read_parquet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_parquet\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_read_feather\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_feather\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mnlee2\\AppData\\Local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mfiona_import_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mnlee2\\AppData\\Local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages\\fiona\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PATH\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpathsep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gdal*.dll\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dll_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'add_dll_directory'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio \n",
    "\n",
    "# Define the arguments directly\n",
    "args = {\n",
    "    'local_folder': './sat/data',\n",
    "    'directory': './sat/outputs',\n",
    "    'fire_fp': r'C:\\Users\\mnlee2\\Desktop\\data\\saved_outputs\\sobol_inputs_2\\inputs=inputs_WIND=ORIGINAL_SPOTTING=.TRUE._S-CONFIG=799-lhs-large_CROWN-FIRE-MODEL=0\\time_of_arrival_0000001_0347428.tif',\n",
    "    'start_date': '2020-09-05',\n",
    "    'end_date': '2020-09-12',\n",
    "    'url': 'https://ftp.wildfire.gov/public/incident_specific_data/calif_s/2020_Incidents/CA-SNF-001391_Creek/IR/NIROPS/',\n",
    "    'viirs_data_path': '/content/drive/My Drive/NASA/data/VIIRS/viirs_complete.csv'\n",
    "}\n",
    "\n",
    "# Simulate argparse functionality by accessing arguments directly\n",
    "class ArgsNamespace:\n",
    "    def __init__(self, args_dict):\n",
    "        for key, value in args_dict.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args_namespace = ArgsNamespace(args)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(args_namespace.local_folder):\n",
    "    os.makedirs(args_namespace.local_folder)\n",
    "\n",
    "if not os.path.exists(args_namespace.directory):\n",
    "    os.makedirs(args_namespace.directory)\n",
    "\n",
    "#Define the variable names\n",
    "local_folder = args_namespace.local_folder\n",
    "directory = args_namespace.directory\n",
    "fire_fp = args_namespace.fire_fp\n",
    "start_date = datetime.strptime(args_namespace.start_date, '%Y-%m-%d')  # Convert to datetime\n",
    "end_date = datetime.strptime(args_namespace.end_date, '%Y-%m-%d')      # Convert to datetime\n",
    "url = args_namespace.url\n",
    "viirs_data_path = args_namespace.viirs_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates Found:\n",
      "2020-09-06\n",
      "2020-09-08\n",
      "2020-09-09\n",
      "2020-09-10\n",
      "2020-09-12\n",
      "2020-09-13\n",
      "2020-09-14\n",
      "2020-09-15\n",
      "2020-09-17\n",
      "2020-09-20\n",
      "2020-09-21\n",
      "2020-09-22\n",
      "2020-09-23\n",
      "2020-09-24\n",
      "2020-09-26\n",
      "2020-09-27\n",
      "2020-09-29\n",
      "2020-10-01\n",
      "2020-10-02\n",
      "2020-10-03\n",
      "2020-10-04\n",
      "2020-10-05\n",
      "2020-10-06\n",
      "2020-10-07\n",
      "2020-10-08\n",
      "2020-10-09\n",
      "2020-10-10\n",
      "2020-10-11\n",
      "2020-10-13\n",
      "2020-10-15\n",
      "2020-10-16\n",
      "2020-10-17\n",
      "2020-10-18\n",
      "2020-10-19\n",
      "2020-10-20\n",
      "2020-10-22\n",
      "2020-10-24\n",
      "2020-10-25\n",
      "2020-10-26\n",
      "2020-10-27\n",
      "2020-10-28\n",
      "2020-10-29\n",
      "2020-10-30\n",
      "2020-10-31\n",
      "2020-11-01\n",
      "2020-11-02\n",
      "2020-11-03\n",
      "2020-11-04\n",
      "2020-11-05\n",
      "2020-11-06\n",
      "2020-11-08\n",
      "2020-11-10\n",
      "2020-11-13\n",
      "Current Date:  2020-09-06\n",
      "No zip file found for date 2020-09-06\n",
      "Current Date:  2020-09-08\n",
      "https://ftp.wildfire.gov/public/incident_specific_data/calif_s/2020_Incidents/CA-SNF-001391_Creek/IR/NIROPS/20200908_OAR/20200908_Creek_IR_shapefiles.zip\n",
      "Files Collected: \n",
      "Current Date:  2020-09-09\n",
      "https://ftp.wildfire.gov/public/incident_specific_data/calif_s/2020_Incidents/CA-SNF-001391_Creek/IR/NIROPS/20200909_OAR/20200909_Creek_IR_shapefiles.zip\n",
      "Files Collected: \n",
      "Current Date:  2020-09-10\n",
      "https://ftp.wildfire.gov/public/incident_specific_data/calif_s/2020_Incidents/CA-SNF-001391_Creek/IR/NIROPS/20200910_OAR/20200910_Creek_IR_shapefiles.zip\n",
      "Files Collected: \n",
      "Current Date:  2020-09-12\n",
      "https://ftp.wildfire.gov/public/incident_specific_data/calif_s/2020_Incidents/CA-SNF-001391_Creek/IR/NIROPS/20200912_OAR/Shape%20Files.zip\n",
      "Files Collected: \n",
      "Error moving file 20200908_Creek_IR_BurnPerimeter.cpg: Destination path './sat/data\\2020-09-08\\20200908_Creek_IR_BurnPerimeter.cpg' already exists\n",
      "Error moving file 20200908_Creek_IR_BurnPerimeter.dbf: Destination path './sat/data\\2020-09-08\\20200908_Creek_IR_BurnPerimeter.dbf' already exists\n",
      "Error moving file 20200908_Creek_IR_BurnPerimeter.prj: Destination path './sat/data\\2020-09-08\\20200908_Creek_IR_BurnPerimeter.prj' already exists\n",
      "Error moving file 20200908_Creek_IR_BurnPerimeter.shp: Destination path './sat/data\\2020-09-08\\20200908_Creek_IR_BurnPerimeter.shp' already exists\n",
      "Error moving file 20200908_Creek_IR_BurnPerimeter.shx: Destination path './sat/data\\2020-09-08\\20200908_Creek_IR_BurnPerimeter.shx' already exists\n",
      "Error moving file 20200909_Creek_IR_BurnPerimeter.cpg: Destination path './sat/data\\2020-09-09\\20200909_Creek_IR_BurnPerimeter.cpg' already exists\n",
      "Error moving file 20200909_Creek_IR_BurnPerimeter.dbf: Destination path './sat/data\\2020-09-09\\20200909_Creek_IR_BurnPerimeter.dbf' already exists\n",
      "Error moving file 20200909_Creek_IR_BurnPerimeter.prj: Destination path './sat/data\\2020-09-09\\20200909_Creek_IR_BurnPerimeter.prj' already exists\n",
      "Error moving file 20200909_Creek_IR_BurnPerimeter.shp: Destination path './sat/data\\2020-09-09\\20200909_Creek_IR_BurnPerimeter.shp' already exists\n",
      "Error moving file 20200909_Creek_IR_BurnPerimeter.shx: Destination path './sat/data\\2020-09-09\\20200909_Creek_IR_BurnPerimeter.shx' already exists\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Goes to website and collects all the nessecary shape files\n",
    "# May have some minor bugs but works enough for rough analysis\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the webpage with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the links on the webpage\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Regular expression to match the folder names\n",
    "pattern = re.compile(r'(\\d{8}_OAR|^\\d{8})/$')\n",
    "\n",
    "##########################\n",
    "print('Dates Found:')\n",
    "# Iterate over the links to print the dates\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "\n",
    "    # If the link matches the pattern\n",
    "    if pattern.match(href):\n",
    "        # Extract the date from the folder name\n",
    "        date_str = re.search(r'\\d{8}', href).group()\n",
    "        date = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "        # Print the date\n",
    "        print(date.strftime('%Y-%m-%d'))\n",
    "###########################\n",
    "zip_found = False\n",
    "# Iterate over the links\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "\n",
    "    # If the link matches the pattern\n",
    "    if pattern.match(href):\n",
    "        # Extract the date from the folder name\n",
    "        date_str = re.search(r'\\d{8}', href).group()\n",
    "        date = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "\n",
    "        # Check if the date is within the range\n",
    "        if start_date <= date <= end_date:\n",
    "            # Print the date\n",
    "            print('Current Date: ', date.strftime('%Y-%m-%d'))\n",
    "\n",
    "            # The URL of the zip file\n",
    "            zip_url = url + href\n",
    "\n",
    "            # Send a GET request to the zip file URL\n",
    "            zip_response = requests.get(zip_url)\n",
    "\n",
    "            # Parse the webpage with BeautifulSoup\n",
    "            zip_soup = BeautifulSoup(zip_response.text, 'html.parser')\n",
    "\n",
    "            # Find all the links on the webpage\n",
    "            zip_links = zip_soup.find_all('a')\n",
    "\n",
    "            # Iterate over the links\n",
    "            for zip_link in zip_links:\n",
    "                zip_href = zip_link.get('href')\n",
    "\n",
    "                # If there are multiple zip files in the folder\n",
    "                if len(zip_links) > 1:\n",
    "                    # Use the pattern that includes 'shape'\n",
    "                    zip_pattern = re.compile(r'.*shape.*\\.zip$', re.IGNORECASE)\n",
    "                else:\n",
    "                    # Use the pattern that matches any zip file\n",
    "                    zip_pattern = re.compile(r'.*\\.zip$')\n",
    "\n",
    "                # If the link matches the pattern\n",
    "                if zip_pattern.match(zip_href):\n",
    "                    # Set the flag to True\n",
    "                    zip_found = True\n",
    "\n",
    "                    # The URL of the zip file\n",
    "                    file_url = zip_url + zip_href\n",
    "                    print(file_url)\n",
    "\n",
    "                    # Send a GET request to the file URL\n",
    "                    file_response = requests.get(file_url)\n",
    "\n",
    "                    # Check if the request was successful\n",
    "                    if file_response.status_code == 200:\n",
    "                        # Open the zip file\n",
    "                        with zipfile.ZipFile(io.BytesIO(file_response.content)) as z:\n",
    "                            # Extract only the files that contain 'HeatPerimeter' or 'BurnPerimeter' in their names\n",
    "                            print('Files Collected: ')\n",
    "                            for file in z.namelist():\n",
    "                                if 'Perimeter' in file or 'BurnPerimeter' in file:\n",
    "                                    z.extract(file, path=os.path.join(local_folder, date.strftime('%Y-%m-%d')))\n",
    "                    else:\n",
    "                        print(f\"Error: Failed to download {file_url}\")\n",
    "\n",
    "            # If no zip file was found, print a message\n",
    "            if not zip_found:\n",
    "                print(f\"No zip file found for date {date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "#############################################################################################################3\n",
    "# Clean up sub-folders and prepare for processing\n",
    "\n",
    "# The path to the data folder\n",
    "data_path = os.path.join(directory, local_folder)\n",
    "data_path = local_folder\n",
    "# Iterate over the dated subfolders\n",
    "for date_folder in os.listdir(data_path):\n",
    "    date_folder_path = os.path.join(data_path, date_folder)\n",
    "\n",
    "    # If the path is a directory\n",
    "    if os.path.isdir(date_folder_path):\n",
    "        # Iterate over the subfolders\n",
    "        for subfolder in os.listdir(date_folder_path):\n",
    "            subfolder_path = os.path.join(date_folder_path, subfolder)\n",
    "\n",
    "            # If the path is a directory\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                # Iterate over the files in the subfolder\n",
    "                for file in os.listdir(subfolder_path):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "                    try:\n",
    "                        # Move the file to the dated parent folder\n",
    "                        shutil.move(file_path, date_folder_path)\n",
    "                    except shutil.Error as e:\n",
    "                        # Handle the exception\n",
    "                        print(f\"Error moving file {file}: {e}\")\n",
    "\n",
    "                # Remove the subfolder\n",
    "                shutil.rmtree(subfolder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--local_folder LOCAL_FOLDER]\n",
      "                             [--directory DIRECTORY] --fire_fp FIRE_FP\n",
      "                             --start_date START_DATE --end_date END_DATE --url\n",
      "                             URL --viirs_data_path VIIRS_DATA_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --start_date, --end_date, --url, --viirs_data_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mnlee2\\AppData\\Local\\anaconda3\\envs\\elmfire_notebook\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3560: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "## Derive the Bounds for the Plots From the Last NIROPS Data of Interest\n",
    "# Get the list of dated subfolders\n",
    "date_folders = [folder for folder in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, folder))]\n",
    "\n",
    "# Parse the dates and find the most recent one\n",
    "ed = max(datetime.strptime(date, '%Y-%m-%d') for date in date_folders).strftime('%Y-%m-%d')\n",
    "print('last date: ', ed)\n",
    "date_folder_path = os.path.join(data_path, ed)\n",
    "\n",
    "# Iterate over the files in the dated subfolder\n",
    "for file in os.listdir(date_folder_path):\n",
    "    # If the file is a shapefile\n",
    "    if file.endswith('.shp'):\n",
    "        file_path = os.path.join(date_folder_path, file)\n",
    "\n",
    "        # Load the shapefile\n",
    "        data = gpd.read_file(file_path)\n",
    "\n",
    "# Assuming `data` is your GeoDataFrame and `geometry` is your geometry column\n",
    "bounds = data.geometry.total_bounds\n",
    "\n",
    "# This will give you a tuple in the format (minx, miny, maxx, maxy)\n",
    "# which corresponds to (min longitude, min latitude, max longitude, max latitude)\n",
    "print(bounds)\n",
    "print(data.head())\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "data.plot(ax=ax, edgecolor='black')\n",
    "\n",
    "# Add title and labels (optional)\n",
    "ax.set_title('Heat Perimeter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIIRS Data Handling\n",
    "# As CSV Files\n",
    "# Read CSV file into a DataFrame\n",
    "df = pd.read_csv(viirs_data_path)\n",
    "\n",
    "# Assuming `df` is your DataFrame and `latitude` and `longitude` are your columns\n",
    "#bounds = [-119.48449481, 36.98991853, -118.97099713, 37.64569761]\n",
    "min_lon, min_lat, max_lon, max_lat = bounds #defined above\n",
    "\n",
    "#Filter the dataframe on the appropriate ROI\n",
    "#Don't Forget to Filter for Aquisition Date Too:\n",
    "\n",
    "df = df[\n",
    "        (df['longitude'] >= min_lon) &\n",
    "        (df['longitude'] <= max_lon) &\n",
    "        (df['latitude'] >= min_lat) &\n",
    "        (df['latitude'] <= max_lat) &\n",
    "        (df['frp'] > 1.0) #threshold for confidence\n",
    "       ]\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "# All the dates are stored here:\n",
    "print(df.shape)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMBINED PLOT\n",
    "#########\n",
    "# This runs with the VIIRS points\n",
    "from pyproj import Transformer\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "\n",
    "# The path to the data folder\n",
    "data_path = local_folder\n",
    "\n",
    "# Get the list of dated subfolders\n",
    "date_folders = [folder for folder in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, folder))]\n",
    "\n",
    "# Sort the dated subfolders in ascending order\n",
    "date_folders.sort()\n",
    "\n",
    "# Get the unique dates from the dataframe\n",
    "unique_dates = df['acq_date'].sort_values().unique()\n",
    "\n",
    "# Combine the dates from the dataframe and the subfolders\n",
    "all_dates = np.unique(np.concatenate((date_folders, unique_dates)))\n",
    "\n",
    "transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:32611\")\n",
    "\n",
    "# start_date = datetime.strptime('2020-09-05', '%Y-%m-%d').date()\n",
    "# end_date = datetime.strptime(ed, '%Y-%m-%d').date()\n",
    "filtered_dates = [date for date in all_dates if start_date <= datetime.strptime(date, '%Y-%m-%d').date() <= end_date]\n",
    "\n",
    "#filtered_dates = [date for date in all_dates if start_date <= datetime.strptime(date, '%Y-%m-%d') <= end_date]\n",
    "\n",
    "# Calculate the number of columns needed for the subplots\n",
    "n_columns = len(filtered_dates)\n",
    "\n",
    "# Define the buffer size (in meters)\n",
    "buffer_size = 5000\n",
    "\n",
    "# Calculate the bounds of the asp data\n",
    "xmin, ymin, xmax, ymax = src.bounds\n",
    "\n",
    "# Apply the buffer to the bounds\n",
    "xmin -= buffer_size\n",
    "ymin -= buffer_size\n",
    "xmax += buffer_size\n",
    "ymax += buffer_size\n",
    "\n",
    "# Open the fire perimeter file\n",
    "with rasterio.open(fire_fp) as fire_src:\n",
    "    # Read the fire perimeter data\n",
    "    fire = fire_src.read(1)\n",
    "\n",
    "    # Mask no_data values\n",
    "    nodata = fire_src.nodata\n",
    "    fire = np.ma.masked_equal(fire, nodata)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "def count_seconds(start_date, start_time, end_date, end_time):\n",
    "    # Format for date and time\n",
    "    format_str = '%Y-%m-%d %H%M'\n",
    "\n",
    "    # Check if start_date and end_date are already datetime objects, if not convert them\n",
    "    if isinstance(start_date, datetime):\n",
    "        start = start_date\n",
    "    else:\n",
    "        # The time string should be padded with zeros if it is not four digits\n",
    "        start = f\"{start_date} {start_time.zfill(4)}\"\n",
    "        start = datetime.strptime(start, format_str)\n",
    "\n",
    "    if isinstance(end_date, datetime):\n",
    "        end = end_date\n",
    "    else:\n",
    "        # The time string should be padded with zeros if it is not four digits\n",
    "        end = f\"{end_date} {end_time.zfill(4)}\"\n",
    "        end = datetime.strptime(end, format_str)\n",
    "\n",
    "    # Calculate the difference in seconds\n",
    "    difference = end - start\n",
    "    return difference.total_seconds()\n",
    "\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axs = plt.subplots(2, n_columns, figsize=(20*n_columns, 16*3))\n",
    "\n",
    "# Iterate over filtered dates\n",
    "for column_idx, date in enumerate(filtered_dates):\n",
    "    # Set column titles\n",
    "    axs[0, column_idx].set_title(date, fontsize=45, pad=20)\n",
    "\n",
    "    # Check if 'date' is a datetime object, if so format it as a string\n",
    "    if isinstance(date, datetime):\n",
    "        date = date.strftime('%Y-%m-%d')\n",
    "\n",
    "    date_folder_path = os.path.join(data_path, date)\n",
    "\n",
    "    # Plot the Aspect (if it should be plotted)\n",
    "    #axs[column_idx].imshow(asp, cmap='viridis')\n",
    "\n",
    "    # If the date exists in the VIIRS dataframe\n",
    "    if date in unique_dates:\n",
    "        # Filter the dataframe for the current date\n",
    "        df_date = df[df['acq_date'] == date].copy()\n",
    "        df_date.loc[:, 'x'], df_date.loc[:, 'y'] = transformer.transform(df_date['latitude'].values, df_date['longitude'].values)\n",
    "\n",
    "        # Get the unique acquisition times\n",
    "        unique_times = df_date['acq_time'].sort_values().unique()\n",
    "\n",
    "        # For each unique time, plot in a separate subplot\n",
    "        for row_idx, acq_time in enumerate(unique_times[:2]):\n",
    "\n",
    "            # Filter the dataframe for the current acquisition time\n",
    "            df_time = df_date[df_date['acq_time'] == acq_time]\n",
    "\n",
    "\n",
    "            #scatter = axs[row_idx][column_idx].scatter(df_time['longitude'], df_time['latitude'], c=df_time['frp'], cmap='hot', alpha=0.5)\n",
    "            asp_downsampled = asp[::10, ::10]  # take every 10th data point in each direction\n",
    "            axs[row_idx][column_idx].imshow(asp_downsampled, cmap='gray', extent=(xmin, xmax, ymin, ymax))\n",
    "\n",
    "            start_time = '0000'\n",
    "            end_date = date\n",
    "            end_time = str(copy(acq_time))\n",
    "            print(start_date, start_time, end_date, end_time)\n",
    "            max_val = count_seconds(start_date, start_time, end_date, end_time)\n",
    "            print('Current Seconds: ', max_val )\n",
    "            fire_masked = np.ma.masked_where(fire > max_val, fire)\n",
    "            #axs[row_idx][column_idx].imshow(fire_masked, cmap='viridis', extent=(xmin, xmax, ymin, ymax))\n",
    "\n",
    "            # Add the scatter points from the dataframe for the current acquisition time\n",
    "            scatter = axs[row_idx][column_idx].scatter(df_time['x'], df_time['y'], c=df_time['frp'], cmap='hot', alpha=0.5, s=100)\n",
    "\n",
    "            # Add a colorbar for the scatter plot\n",
    "            #fig.colorbar(scatter, ax=axs[row_idx][column_idx], label='FRP')\n",
    "\n",
    "            # Set the title for the subplot to the acquisition time\n",
    "            axs[row_idx][column_idx].set_title(f\"VIIRS Acquisition Time: {acq_time}\", fontsize=36)\n",
    "\n",
    "    # If the date exists in the subfolders\n",
    "    if date in date_folders:\n",
    "        # The path to the shapefile\n",
    "        date_folder_path = os.path.join(data_path, date)\n",
    "\n",
    "        # Iterate over the files in the dated subfolder\n",
    "        for file in os.listdir(date_folder_path):\n",
    "            # If the file is a shapefile\n",
    "            if file.endswith('.shp'):\n",
    "                file_path = os.path.join(date_folder_path, file)\n",
    "\n",
    "                # Load the shapefile\n",
    "                data = gpd.read_file(file_path)\n",
    "                data = data.to_crs(\"EPSG:32611\")\n",
    "\n",
    "                # Plot the most recent NIROPS perimeter on all subplots for the current date\n",
    "                for row_idx in range(2):\n",
    "                    data.plot(ax=axs[row_idx][column_idx], edgecolor='black', alpha=0.4)\n",
    "\n",
    "                    # # Set the limits of x-axis and y-axis\n",
    "                    axs[row_idx][column_idx].set_xlim(xmin, xmax)\n",
    "                    axs[row_idx][column_idx].set_ylim(ymin, ymax)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "# This runs with the VIIRS points\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "\n",
    "# The path to the data folder\n",
    "data_path = local_folder\n",
    "\n",
    "# Get the list of dated subfolders\n",
    "date_folders = [folder for folder in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, folder))]\n",
    "\n",
    "# Sort the dated subfolders in ascending order\n",
    "date_folders.sort()\n",
    "\n",
    "# Ensure 'time' is in datetime format\n",
    "gdf_viirs_perim['time'] = pd.to_datetime(gdf_viirs_perim['time'])\n",
    "\n",
    "# Get the unique dates from the gdf_viirs_perim\n",
    "# Extract date and hour from 'time' column\n",
    "gdf_viirs_perim['date'] = gdf_viirs_perim['time'].dt.date.astype(str)\n",
    "gdf_viirs_perim['hour'] = gdf_viirs_perim['time'].dt.hour.astype(str)\n",
    "\n",
    "# Get unique dates and hours\n",
    "unique_dates = gdf_viirs_perim['date'].unique()\n",
    "unique_hours = gdf_viirs_perim['hour'].unique()\n",
    "\n",
    "unique_dates = unique_dates.astype(str)\n",
    "\n",
    "# Combine the dates from the gdf_viirs_perim and the subfolders\n",
    "#all_dates = np.unique(np.concatenate((date_folders, unique_dates)))\n",
    "all_dates = np.unique(np.concatenate((date_folders, unique_dates.astype(str))))\n",
    "\n",
    "print(all_dates)\n",
    "\n",
    "# Filter dates based on range\n",
    "end_date = ed\n",
    "end_date = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "\n",
    "# Filter dates based on range\n",
    "if isinstance(start_date, str):\n",
    "  start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "\n",
    "print('start: ', start_date)\n",
    "print('end: ', end_date)\n",
    "\n",
    "filtered_dates = [date for date in all_dates if start_date <= datetime.strptime(date, '%Y-%m-%d').date() <= end_date]\n",
    "\n",
    "# Do something elseend_date = datetime.strptime('2020-09-08', '%Y-%m-%d').date()\n",
    "#filtered_dates = [date for date in all_dates if start_date <= datetime.strptime(date, '%Y-%m-%d').date() <= end_date]\n",
    "\n",
    "# The rest of the code is identical until the loop where it checks if the date exists in the dataframe\n",
    "# Create a grid of subplots\n",
    "\n",
    "# Calculate the number of columns needed for the subplots\n",
    "n_columns = len(filtered_dates)\n",
    "\n",
    "# Define the buffer size (in meters)\n",
    "buffer_size = 5000\n",
    "\n",
    "# Calculate the bounds of the asp data\n",
    "xmin, ymin, xmax, ymax = src.bounds\n",
    "\n",
    "# Apply the buffer to the bounds\n",
    "xmin -= buffer_size\n",
    "ymin -= buffer_size\n",
    "xmax += buffer_size\n",
    "ymax += buffer_size\n",
    "\n",
    "# Open the fire perimeter file\n",
    "with rasterio.open(fire_fp) as fire_src:\n",
    "    # Read the fire perimeter data\n",
    "    fire = fire_src.read(1)\n",
    "\n",
    "    # Mask no_data values\n",
    "    nodata = fire_src.nodata\n",
    "    fire = np.ma.masked_equal(fire, nodata)\n",
    "\n",
    "#fig, axs = plt.subplots(2, n_columns, figsize=(20*n_columns, 16*3))\n",
    "\n",
    "# Iterate over filtered dates\n",
    "print('filtered dates')\n",
    "print(filtered_dates)\n",
    "for column_idx, date in enumerate(filtered_dates):\n",
    "    print(date)\n",
    "    # Set column titles\n",
    "    axs[0, column_idx].set_title(date, fontsize=45, pad=20)\n",
    "\n",
    "    # Check if 'date' is a datetime object, if so format it as a string\n",
    "    if isinstance(date, datetime):\n",
    "        date = date.date()\n",
    "\n",
    "    # If the date exists in the gdf_viirs_perim\n",
    "    if date in unique_dates:\n",
    "        print(date)\n",
    "        # Filter the gdf_viirs_perim for the current date\n",
    "        gdf_date = gdf_viirs_perim[gdf_viirs_perim['date'] == date]\n",
    "\n",
    "        print(gdf_date.head())\n",
    "        # No need to transform coordinates, as gdf_viirs_perim is already in correct CRS\n",
    "        gdf_date = gdf_date.to_crs(\"EPSG:32611\")\n",
    "\n",
    "        # Instead of plotting scatter points, plot the geometry directly\n",
    "        # For each unique hour\n",
    "        print('cur date: ', date)\n",
    "        for row_idx, hour in enumerate(unique_hours):\n",
    "\n",
    "\n",
    "            # Filter the gdf_date for the current hour\n",
    "            gdf_time = gdf_date[gdf_date['hour'] == hour]\n",
    "\n",
    "            # Convert gdf_time to correct CRS\n",
    "            gdf_time = gdf_time.to_crs(\"EPSG:32611\")\n",
    "\n",
    "            # Instead of scatter plot, plot the geometry directly\n",
    "            #asp_downsampled = asp[::10, ::10]  # take every 10th data point in each direction\n",
    "            axs[row_idx][column_idx].imshow(asp_downsampled, cmap='gray', extent=(xmin, xmax, ymin, ymax))\n",
    "\n",
    "            start_time = '0000'\n",
    "            end_date = date\n",
    "            end_time = str(copy(hour)) +'00'\n",
    "            print(start_date, start_time, end_date, end_time)\n",
    "\n",
    "            max_val = count_seconds(start_date, start_time, end_date, end_time)\n",
    "            print('hour: ', hour)\n",
    "            print('Current Seconds: ', max_val )\n",
    "            fire_masked = np.ma.masked_where(fire > max_val, fire)\n",
    "            axs[row_idx][column_idx].imshow(fire_masked, cmap='viridis', extent=(xmin, xmax, ymin, ymax))\n",
    "\n",
    "            gdf_time.plot(ax=axs[row_idx][column_idx], color='red', alpha=0.5)\n",
    "\n",
    "            # Set the title for the subplot to the hour\n",
    "            axs[row_idx][column_idx].set_title(f\"VIIRS Acquisition Time: {hour}\", fontsize=36)\n",
    "\n",
    "# After the creation of the subplots add these lines for column titles:\n",
    "for ax, col in zip(axs[0], filtered_dates):\n",
    "    ax.annotate(col, xy=(0.5, 1), xytext=(0, 40),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                ha='center', va='baseline', fontsize=40)\n",
    "\n",
    "# And these lines for the row titles:\n",
    "for ax, row in zip(axs[:,0], range(1, 3)):\n",
    "    ax.annotate(f'VIIRS Overpass # {row}', xy=(0, .5), xytext=(-ax.yaxis.labelpad - 5, 0),\n",
    "                xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                ha='right', va='center', fontsize=40)\n",
    "\n",
    "\n",
    "fig.suptitle('ELMFIRE Model Simulations overlayed with available VIIRS + NIROPS data + Chen et al (2020) VIIRS fire perimeters', fontsize=85, y=-0.03)\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Create the artists for the legend\n",
    "# Create the artists for the legend with thicker lines\n",
    "nirops_artist = mlines.Line2D([], [], color='blue', marker='_', markersize=15, linewidth=20, label='NIROPS')\n",
    "viirs_perimeter_artist = mlines.Line2D([], [], color='red', marker='_', markersize=15, linewidth=20, label='VIIRS fire perimeter from (Chen et al. 2020)')\n",
    "viirs_frp_artist = mlines.Line2D([], [], color='black', linestyle='None', marker='.', markersize=60, label='raw VIIRS FRP')\n",
    "fire_perimeter_artist = mlines.Line2D([], [], color='green', marker='_', markersize=15, linewidth=20, label='fire perimeter')\n",
    "\n",
    "# Create a list of the artists\n",
    "artists = [nirops_artist, viirs_perimeter_artist, viirs_frp_artist, fire_perimeter_artist]\n",
    "\n",
    "# Create the legend\n",
    "plt.figlegend(handles=artists, loc='lower center', ncol=2, fontsize=100, bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(os.path.join(directory, 'spread_plot.png'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmfire_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
